# The phases of a project {#phases}

Every project is different and the exact way you approach it may vary. That said, we have found that nearly every project has some aspect of six major phases. We outline these in the sections below and expand on two of those in detail in Chapters \@ref(definition) and \@ref(execution). While you may find that you do not need to formally demarcate each phase explicitly, we suggest that you go through the process of considering each carefully during the course of your project.  

<br/>

::: {.center}

```{r bottom-fig, fig.cap='(ref:fig2)', out.width='100%', fig.show='hold', echo=FALSE}
knitr::include_graphics(rep('figures/Figure_2-phases.png', 1))
```

(ref:fig2) **Six phases of a project.** When designing and delivering a project, we find it useful to break the process down into these major phases. 

:::

=======
:::{.infobox}
**Data access and security**

Getting access to data can often be a challenge due to security policies that may be in place. This is especially common in financial services, but is certainly not limited to this industry. There are many ways of making data access secure so you may have to work with the data administrators to work out a safe and secure plan that works for them. We have each had to endure inconveniences to access data, such as sitting in data-secure rooms with no internet access, having encrypted laptops sent to us to use for our work or having had to access data via VPNs and cloud-based servers that restricted access based on multi-factor authentication. For some industries, we have even had to go through extensive background checks. There are often quite a few hurdles to jump through when it comes to data security so we recommend discussing this with your client early in order to get these hurdles out of the way.
:::

What defines clean data can vary greatly across professions and industries. For some people, hundreds of differently structured excel files that are human-readable would be considered clean data, yet for data scientists this would be a nightmare, requiring a substantial investment to merge all of the files for them to be analysed as a coherent whole. Databases or data warehouses, in contrast, tend not to suffer from such problems of *data structure*. However, even well-structured data can require substantial cleaning. For example the dataset could contain duplicate entries, missing values or inconsistently-recorded measurements, to name but a few potential data cleaniness problems. While exploring and working with a poorly-structured or unclean dataset may be agonising, knowing the state of the data that you will use for the project is critical in order to accurately appreciate the scale of the prospective project. Unexpected data cleaning can take weeks and will invariably cause delays to your project, so this insight will allow you to budget appropriately when creating your project plan.

The next step is reducing some of the uncertainty and risk by carrying out hypothesis testing. What do we mean by that? In one way, we mean statistical hypothesis testing in the truest sense of the word: formulating null and alternative hypotheses and applying appropriate statistical tests to see how well the evidence supports them. However, we also mean testing more general ideas about the data, usually based on anecdotal evidence or domain experience from those who know the business the best. For example, your client may be confident that sales are highest on Fridays -- use that as the basis for an experiment in your EDA and see if the data really do support that notion. Often with such assumptions, the data will indeed confirm what your client *knows* (or, more accurately, *thinks they know*) about the data. But sometimes those assumptions are wrong, and you are unable to find support for the assumption **in the data you have**. 

It is important to note here that hypothesis tests that fail to support your client's assumptions do not necessarily mean that those assumptions are false. We emphasised the words "in the data you have" in the preceding paragarph for a reason: your data may not be representative of reality. This touches on a fundamental notion in data science -- that data is not the same as ground truth. Data are artefacts and are affected by biases in collection methodology, study design, storage, aggregation and interpretation. In short, the data that comes to you has been affected by the many design choices that were made by others -- choices about every step on the journey between the real world and the number you see in front of you. No dataset is ever completely unbiased. For an in-depth discussion of this topic, we stonlgy recommend [this article](https://medium.com/@angebassa/data-alone-isnt-ground-truth-9e733079dfd4){target="_blank"} by Angela Bassa.

Does this mean that data science is worthless? Of course not. But it does mean that you should keep in mind that some bias will be baked into every dataset. You job is to identify where biases may lay and account for them in your mission to "reach a factual understanding of truth" [@bassa_2017]. In an ideal world, you (the data scientist) would have a say in how data are collected and curated, therefore allowing you to understand what design choices were made. In reality this almost never happens, leaving you to work with data that has been affected by the choices of others. During this phase of the project, you will want to validate some of the foundational assumptions upon which your project will be based. Our advice is to do this as scientifically as possible so that you can maximise your chances of accurately assessing what will be involved to deliver the project objectives. 

We recommend that you time-box this phase of the project. Exactly how much time you spend will vary: in some cases a few hours of exploring the data may be sufficient, in others you may want to spend several weeks going through a formal in-depth EDA and data validation activity. Time-boxing allows the team to focus their efforts on the most pressing concerns without starting to implement the entire project. Remember, **the goal is not to get started with producing results per se, but rather to reduce risk and explore possible solutions.** In software engineering, this is often done in the form of a design sprint. In data science, we can also work in sprints but the process is slightly different as we need to validate a hypothesis -- the process is far more scientific. Doing this work beforehand helps us drastically reduce uncertainties and ensures that projects which are unlikely to succeed don’t get started; if we discover there is insufficient or insignificant data to reach the goal then we can pause the analysis in order to collect more suitable data, delaying the project until its goals are achievable.

The reader might think that doing this phase is hindering the project – delaying it from really getting started and providing little in the way of tangible outcomes. In our experience, this is untrue: when we scope out a project we can more accurately predict what the outcome will be and what resources are needed to achieve the end goal. This translates into less risk for clients and data scientists alike. Being able to quickly demonstrate progress or build a basic proof-of-concept reassures all stakeholders that the project will be successful. For the data scientist, the lower levels of risk can help us to more-confidently estimate the time required for the remainder of the work, thereby avoiding the temptation to add budget padding to defend against unexpected obstacles. We have personally had a 100% success rate of winning project proposals when we have included this stage as a discrete engagement. The reason is clear -- having had time to scope out the project, we could write proposals that were more competitive because they lacked the padding often used to protect against project uncertainty.

:::{.infobox}
**Using science with models**

It is essential to note that general engineering projects differ from data science projects. Data science projects need to be scientifically correct and for this, you need to follow a scientific process. It is not enough to make a library call to train a model and assume it will perform a certain way just because you tested a few cases. Models you create need to be robust or you will get large varying results that cannot be explained. Making models robust to outliers is an important step in modelling. So be warned, it is not enough for somebody to have used a machine learning library in the past, they should be capable of following a scientific process required to build what is necessary for a data science project to succeed.
:::

In this phase, it is also important to start thinking about you will test your final solution. Can you use TDD (test-driven development)? This strategy requires you to set up extensive test cases now that you will use to validate your final solution later. Doing this will let you think through the problem more clearly and write down what criteria the result will adhere to.

The last thing to bear in mind in this phase is that you might not get the desired outcome. It is essential that you allow the scoping project to fail if need be. At the very least you saved yourself from failing at a larger scale. Most probably you will discover shortcomings that will not allow the project to be successful. How can these issues be addressed? Can more data be collected? Can the data be enriched with another source? ) or can data be cleaned even further to get a better sample? A valid outcome is to make a good recommendation as to how the project can still be successful by doing other work. For example, can clickstream data be collected from the website if there are insufficient orders to make decent recommendations for customers who have never bought an item? This would solve the cold start problem many recommendation engines face.


## Phase 4: Project definition
In the previous scoping section we got to do some hands-on work. At this point, you have understood the business case for your project and had a look at the data that will support the work. You will also have defined what a successful outcome is, established metrics for measuring success and developed a clear understanding of the complexity of the task in front of you. The next steps are to concretely define the project, describe it in the form of a written proposal and put a price tag on it. Welcome to Phase 4: Project definition.

This is a difficult task and it comes with a large number of considerations that you need to take into account. We have, therefore, put a detailed description of how we approach this in a subsequent chapter. Here we highlight a few of those factors.

When defining your project, it’s good to start with a project plan that can be broken down into milestones. Think about what the objective of each milestone is, and what deliverable will mark its completion. What skills will be needed for the work? If you are building a team, you’ll want to make sure they have what’s required; if you’re working on your own, will you have to do any homework to ensure that you have the experience that is needed. You will have to put a price on the work and think about how the project will be managed. These items are discussed in detail in Chapter \@ref(definition). 

## Phase 5: Exploration, execution and pivots
This is probably the most exciting phase as we have come to the actual execution of the project. Firstly we will start by doing research, the research sets the groundwork for the entire project. It is here that we discover what works and what doesn’t. We try different approaches, weighing the pros and cons of each before selecting a final solution. 

This phase is different from the scoping done previously as now the goal is to develop and deliver on the full scope. The scoping exercise only delivered a good idea on what the solution might look like, not necessarily an optimum one. In this phase, we are looking for a solution that meets all of the requirements. To elaborate, the scoping exercise might have produced a model capable of predicting good results but not necessarily at a speed or larger scale required for production.

Once we have made these decisions and agreed on them with the client/manager we are ready to start building. If however, you do not get agreement at this point, have a longer session to flesh out any other requirements.

In this step,  we go deeper into the chosen approach. Deeper may mean using all of the data instead of just a sample, improve feature engineering, improve model accuracy, do a deeper analysis or even build a viable usable product.

Throughout this process, we will evaluate our results and possibly move back to the building stage or even the research stage if we do not have adequate results. Client engagement is critical and we will include them regularly. For further details on these stages see Chapter \@ref(execution).

## Phase 6: Handover and knowledge transfer (aka finding a new owner)
Sometimes this phase is really simple. If the deliverable was a report then nothing further needs to be done. For more complex projects this, however, can be the most difficult phase. 

If you are creating a system, remember to deploy the solution according to the plan. It is often not enough to hand over source code as crucial details are often in the deployment. Does your solution scale? Make sure to stress test your application so that it doesn’t crash unexpectedly.

For the long term success of any project, there must be a long term strategy in place. The software will always need to be maintained and depending on what you developed models might ingest new data and the results will slowly start to drift over time. How will the organisation deal with this? Is there a plan in place to maintain these models over time and who will do this?

::: {.infobox}
**The concept ‘drift’ explained**

In the most challenging of data analysis applications, data evolves and must be analyzed in near real-time. Patterns and relations in such data often evolve, thus, models built for analyzing such data quickly become obsolete over time. In machine learning and data mining, this phenomenon is referred to as drift.

To combat drift, a model should be thoroughly analysed and a decision needs to be made on how frequently a model should be re-trained in order to remain relevant.
:::

Oftentimes those that develop a system are not those that need to maintain it long term. This by itself might cause many issues but to make sure this goes as smoothly as possible be sure to write clear concise code, following coding standards, writing detailed documentation and having decent test coverage. Regarding documentation, it is best practice to write your comments and documentation alongside the actual code. Doing this will mean your documentation is more likely to stay up to date. If documentation is in a different location than the code, then a developer making changes, would also need to remember to update the documentation. 

Many great tools can generate beautiful looking documentation from comments added in your code. For python, we can highly recommend [Sphynx](https://www.sphinx-doc.org/en/master/){target="_blank"} as it is widely used and gives you loads of great styles to choose from. 
Regarding proper coding style, we recommend you follow [PEP8](https://www.python.org/dev/peps/pep-0008/){target="_blank"} for python and for R, the [R style guide](http://adv-r.had.co.nz/Style.html){target="_blank"}. We recommend you install a linter into your text editor so that your code will be constantly checked against these coding standards. Coding standards make your code far more readable so it is recommended to adhere to them as much as possible, We also find it useful to use a formatter, for python we use [Black](https://github.com/psf/black){target="_blank"}. This will automatically format your python code to meet the PEP8 guidelines. You will never need to worry about code formatting again as a formatter can automatically add the correct spacing, line breaks and other coding standards. It is, however, important to note that once you use a formatter, you can’t realistically override the formatting and you will need to accept some style changes as is. Every time you run your formatter these changes will be undone so there is no point sticking to your preferred format.

When you reach the end of your project, find the person responsible for maintaining the system and run through how it works. Do a complete handover running through both the code and the documentation, adding comments and notes to clarify points that are not clear to the person you are handing over to.

If handover to an actual person is not possible for whatever reason, eg. if a permanent data scientist is not yet hired, then make sure you make your documentation as clear as possible. Do not leave it to chance and write it as if you are writing for a very junior person. More documentation is recommended in these cases.

Make sure the test cases are understood and implemented in such a way that they are run regularly. There are plenty of continuous integration tools that do this for you. This will stop any performance altering changes from taking place. To avoid the negative consequences of model drift, recalculate the model performance on new data regularly.

## Chapter summary
In this chapter, we have given you a lot of information about how we approach the projects that we design and execute. We hope that you find our suggestions useful. Naturally, projects come in different sizes and scopes, and whether or not all of these phases are appropriate for a given project depends on many factors. 

While we have presented project evaluation and project phases as different concepts, in actuality they go hand-in-hand. We encourage you to think about the levels of project evaluation as you go through the phases of project design. As we saw in Chapter \@ref(levels), the levels of project evaluation are hierarchical. They also have a temporal component, by which mean that one should think about the higher levels during the earlier phases of a project when the business case is being discussed. The lower levels, such as the *project process* phase come more into consideration during the later phases of scoping, execution and handover. To illustrate this point, we have combined the earlier two figures into the figure before.

(ref:fig3) Combined project evaluation and phases

::: {.center}

```{r full-figure1, fig.cap='(ref:fig3)', fig.show='hold', message=FALSE, out.width='100%', echo=FALSE}
knitr::include_graphics(rep('figures/Framework phases.png', 1))
```

:::
